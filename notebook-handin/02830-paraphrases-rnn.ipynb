{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "paraphrases.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kuJC90SAzuCA"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mQEFknbyRnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7NJckmrROgo",
        "colab_type": "code",
        "outputId": "27ba8414-5809-4c40-8d1b-ef10d7d44e11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip3 install pytorch-nlp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhF7dACVyRnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWY4-oAkyWxj",
        "colab_type": "code",
        "outputId": "4e04fda5-dc01-4844-9e20-509edaee3c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4q6YU9GyRnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "google_path = '/content/gdrive/My Drive/colab/paraphrasing/'\n",
        "dataset_path = google_path + 'paraphrases.txt'\n",
        "picked_dataset_path = google_path + \"paraphrases_dataset.pickle\"\n",
        "encoder_path = google_path + 'encoder.model'\n",
        "decoder_path = google_path + 'decoder.model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtKV3xIrzFvk",
        "colab_type": "text"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjvCiDaxyRn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalize(s):\n",
        "    s = re.sub(\"ß\", \"ss\", s)\n",
        "    s = re.sub(\"ä\", \"ae\", s)\n",
        "    s = re.sub(\"ö\", \"oe\", s)\n",
        "    s = re.sub(\"ü\", \"ue\", s)\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEdu99dCyRn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 40\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK4VONZtyRn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"<PAD>\", SOS_token: \"<SOS>\", EOS_token: \"<EOS>\"}\n",
        "        self.n_words = 3  # Count SOS and EOS\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W60iqBBPyRn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(limit=10000, reverse=True):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(dataset_path).read().strip().split('\\n')\n",
        "    print('read %s lines' % len(lines))\n",
        "    lines = lines[:limit]\n",
        "    \n",
        "    pairs = [l.split('|||') for l in lines]\n",
        "    pairs = [(x[1], x[2]) for x in pairs]\n",
        "    pairs = [(normalize(a), normalize(b)) for a,b in pairs]\n",
        "\n",
        "    #pairs = filter_pairs(pairs)\n",
        "    \n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    \n",
        "    vocab = Vocab()\n",
        "    for pair in pairs:\n",
        "        vocab.add_sentence(pair[0])\n",
        "        vocab.add_sentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(vocab.n_words)\n",
        "    return vocab, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vzLF-uvzh-D",
        "colab_type": "text"
      },
      "source": [
        "# Convert to Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHTbaqf6yRoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def tensor_to_string(tensor, vocab, ignore_index=-1):\n",
        "    tokens = [vocab.index2word[idx.item()] for idx in tensor if idx != ignore_index]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def sentence_to_index(vocab, sentence):\n",
        "    return [vocab.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def sentence_to_tensor(vocab, sentence):\n",
        "    indexes = sentence_to_index(vocab, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def pair_to_tensor(pair):\n",
        "    src, tar = pair\n",
        "    input_tensor = sentence_to_tensor(vocab, src)\n",
        "    target_tensor = sentence_to_tensor(vocab, tar)\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def make_dataset(pairs, split_ratio=0.8):\n",
        "    # Shuffle dataset\n",
        "    n = len(pairs)\n",
        "    indices = np.arange(n)\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    # Convert to tensors, use indices\n",
        "    tensor_pairs = [pair_to_tensor(pairs[i]) for i in indices]\n",
        "    \n",
        "    # Split dataset\n",
        "    split_idx = int(split_ratio * n)\n",
        "    train_data = tensor_pairs[:split_idx]\n",
        "    val_data = tensor_pairs[split_idx:]\n",
        "    return train_data, val_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xcHl5gEyRoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_dataset(train_data, val_data, vocab):\n",
        "    dataset = (train_data, val_data, vocab)\n",
        "    pickle.dump(dataset, open(picked_dataset_path, \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjgtV2c1zmEj",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B-iJDgkyRoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''ENCODER NETWORK'''\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        x = self.embedding(input).view(1, 1, -1)\n",
        "        output, h = self.gru(x, hidden)\n",
        "        return output, h\n",
        "\n",
        "    def init_hidden_state(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "'''DECODER NETWORK'''\n",
        "class AttentionDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "        \n",
        "        # Set dims\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Set layers\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attention_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # Embed\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Compute attention score as matrix product\n",
        "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "        \n",
        "        # Linear + RELU\n",
        "        x = torch.cat((embedded[0], attention_applied[0]), 1)\n",
        "        x = self.attention_combine(x).unsqueeze(0)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # GRU RNN\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        \n",
        "        # Softmax over output to transform into word probability\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        \n",
        "        return output, hidden, attention_weights\n",
        "\n",
        "    def init_hidden_state(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUvJy-G2BJ7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src, tar = train_data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK7GKTgoCJF6",
        "colab_type": "code",
        "outputId": "159193ee-85b8-4bde-fabf-86490efd89c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "src[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L-QKhXGBQC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "encoder = EncoderRNN(vocab.n_words, hidden_size).to(device)\n",
        "decoder = AttentionDecoderRNN(hidden_size, vocab.n_words, dropout_p=0.5).to(device)\n",
        "\n",
        "# Init optimizers\n",
        "enc_opt = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "dec_opt = optim.Adam(decoder.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSNU0EY-zpt1",
        "colab_type": "text"
      },
      "source": [
        "# Unrolling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKxRVeayyRoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_unrolling(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    loss = 0\n",
        "\n",
        "    # Set optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Calculate sequence lengths\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "    \n",
        "    # Initialize encoder output and hidden state as the zero vectors\n",
        "    encoder_hidden = encoder.init_hidden_state()\n",
        "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "    \n",
        "    # Unroll Encoder RNN\n",
        "    for t in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[t], encoder_hidden)\n",
        "        encoder_outputs[t] = encoder_output[0, 0]\n",
        "\n",
        "    # After unrolling the Encoder RNN, the decoder takes the last encoder \n",
        "    # hidden state as it's initial hidden state\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    # Unroll Attention Decoder RNN, stop when most probable output token is the EOS token\n",
        "    for dt in range(target_length):\n",
        "        decoder_output, decoder_hidden, decoder_att = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "        loss += criterion(decoder_output, target_tensor[dt])\n",
        "        \n",
        "        # Get index of the most probable token\n",
        "        _, argmax = decoder_output.topk(1)\n",
        "        predicted_next_output = argmax.squeeze().detach()\n",
        "        \n",
        "        # Apply teacher forcing with probability 0.5\n",
        "        # Teacher forcing feeds the target token to the decoder\n",
        "        # rather than the predicted next token\n",
        "        teacher_forcing = np.random.random() > 0.5\n",
        "        if teacher_forcing: \n",
        "            decoder_input = target_tensor[dt]\n",
        "        else:\n",
        "            decoder_input = predicted_next_output\n",
        "        \n",
        "        # Stop unrolling if token is EOS\n",
        "        if predicted_next_output.item() == EOS_token: \n",
        "            break\n",
        "\n",
        "    # Perform BackProp\n",
        "    loss.backward()\n",
        "\n",
        "    # Tune params\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKcZyiOZyRoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def as_minutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def time_since(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfVuhsYJyRoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_one_epoch(pairs, encoder, decoder, start, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    \n",
        "    # Init loss function, ignore PAD tokens\n",
        "    criterion = nn.NLLLoss(ignore_index=PAD_token)\n",
        "    n_iters = len(pairs)\n",
        "    \n",
        "    for i in range(1, n_iters + 1):\n",
        "        # Split training pair\n",
        "        input_tensor, target_tensor = pairs[i -1]\n",
        "\n",
        "        # Run through train algorithm\n",
        "        loss = rnn_unrolling(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        \n",
        "        # Add losses to aux. variables\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print('%s (%d %d%%) %.4f' % (time_since(start, i / n_iters), i, i / n_iters * 100, print_loss_avg))\n",
        "            print_loss_total = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuJC90SAzuCA",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smZEIiAuXFuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchnlp.metrics import get_moses_multi_bleu\n",
        "\n",
        "def compute_bleu_score(vocab, target_tensor, pred_tensor):\n",
        "    # Convert tensors to strings\n",
        "    target = tensor_to_string(target_tensor, vocab, ignore_index=PAD_token)\n",
        "    prediction = tensor_to_string(pred_tensor, vocab, ignore_index=PAD_token)\n",
        "\n",
        "    target = [' '.join(target)]\n",
        "    prediction = [' '.join(prediction)]\n",
        "    \n",
        "    # Compute BLEU score with the official BLEU perl script\n",
        "    score = get_moses_multi_bleu(prediction, target, lowercase=True)\n",
        "    return score\n",
        "\n",
        "def evaluate(encoder, decoder, input_tensor):\n",
        "    with torch.no_grad():\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.init_hidden_state()\n",
        "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
        "\n",
        "        # Unroll Encoder RNN\n",
        "        for t in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[t], encoder_hidden)\n",
        "            encoder_outputs[t] += encoder_output[0, 0]\n",
        "\n",
        "        # Initialize Decoder Hidden State as the last Encoder Hidden State\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
        "\n",
        "        # Unroll Decoder until most probable tokens is EOS\n",
        "        for dt in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[dt] = decoder_attention.data\n",
        "\n",
        "            # Get index of most probable token (argmax)\n",
        "            _, argmax = decoder_output.data.topk(1)\n",
        "            decoded_words.append(argmax)\n",
        "            \n",
        "            # Stop unrolling if EOS token is most probable\n",
        "            if argmax.item() == EOS_token:\n",
        "                break\n",
        "            # Stop keeping track of gradients..? i.e detach() function\n",
        "            decoder_input = argmax.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:dt + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a-uuw8RzzzI",
        "colab_type": "text"
      },
      "source": [
        "# Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4gv_cU5yRoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_loop(n_epochs=5):\n",
        "    start = time.time()\n",
        "    for epoch in range(n_epochs):\n",
        "        # Shuffle indices and data\n",
        "        indices = np.arange(len(train_data))\n",
        "        np.random.shuffle(indices)\n",
        "        shuffled_data = [train_data[i] for i in range(len(train_data))]\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_one_epoch(train_data, encoder1, decoder1, start, print_every=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42NSHR-9z1XH",
        "colab_type": "text"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZJNd1aQyRoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model_params(encoder, decoder):\n",
        "    torch.save(encoder.state_dict(), google_path + 'encoder.model')\n",
        "    torch.save(decoder.state_dict(), google_path + 'decoder.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaNXy-wHyRoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def move_data_to_device(data):\n",
        "    data = [(src.to(device), tar.to(device)) for src,tar in data]\n",
        "    return data\n",
        "\n",
        "# Pad AFTER Loading, since otherwise we are saving a lot of zeros on disk\n",
        "def pad_pairs(tensor_pairs):\n",
        "    # Pad tensors with 0s\n",
        "    srcs = [src for src,tar in tensor_pairs]\n",
        "    tars = [tar for src,tar in tensor_pairs]\n",
        "    srcs_padded = pad_sequence(srcs, batch_first=True)\n",
        "    tars_padded = pad_sequence(tars, batch_first=True)\n",
        "    return list(zip(srcs_padded, tars_padded))\n",
        "\n",
        "def load_dataset(from_scratch=False):\n",
        "    if from_scratch:\n",
        "        vocab, pairs = prepare_data(10000)\n",
        "        train_data, val_data = make_dataset(pairs)\n",
        "    else:\n",
        "        train_data, val_data, vocab = pickle.load(open(picked_dataset_path, \"rb\"))\n",
        "    \n",
        "    # Move data to device (gpu or cpu)\n",
        "    train_data = move_data_to_device(train_data)\n",
        "    val_data = move_data_to_device(val_data)\n",
        "    \n",
        "    # Pad pairs\n",
        "    train_data = pad_pairs(train_data)\n",
        "    val_data = pad_pairs(val_data)\n",
        "    \n",
        "    return train_data, val_data, vocab\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE2wV76Fz4VB",
        "colab_type": "text"
      },
      "source": [
        "# Start Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaT4dnpRyRoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, val_data, vocab = load_dataset(from_scratch=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au5uB59kyRof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model():\n",
        "    # Load trained model params\n",
        "    encoder_dict = torch.load(encoder_path, map_location=torch.device(device))\n",
        "    decoder_dict = torch.load(decoder_path, map_location=torch.device(device))\n",
        "\n",
        "    # Create untrained model\n",
        "    hidden_size = 256\n",
        "    enc = EncoderRNN(vocab.n_words, hidden_size).to(device)\n",
        "    dec = AttentionDecoderRNN(hidden_size, vocab.n_words, dropout_p=0.5).to(device)\n",
        "    \n",
        "    # Put weights into models\n",
        "    enc.load_state_dict(encoder_dict)\n",
        "    dec.load_state_dict(decoder_dict)\n",
        "\n",
        "    # Init optimizers with Stochastic Gradient Descent\n",
        "    encoder_optimizer = optim.Adam(enc.parameters(), lr=0.001)\n",
        "    decoder_optimizer = optim.Adam(dec.parameters(), lr=0.001)\n",
        "\n",
        "    return enc, dec, encoder_optimizer, decoder_optimizer\n",
        "\n",
        "def get_model(load_pretrained=False):\n",
        "    # Init loss function, ignore PAD tokens\n",
        "    criterion = nn.NLLLoss(ignore_index=PAD_token)\n",
        "    \n",
        "    if load_pretrained:\n",
        "        enc, dec, enc_opt, dec_opt = load_model()\n",
        "    else:\n",
        "        hidden_size = 256\n",
        "        enc = EncoderRNN(vocab.n_words, hidden_size).to(device)\n",
        "        dec = AttentionDecoderRNN(hidden_size, vocab.n_words, dropout_p=0.5).to(device)\n",
        "\n",
        "        # Init optimizers\n",
        "        enc_opt = optim.Adam(enc.parameters(), lr=0.001)\n",
        "        dec_opt = optim.Adam(dec.parameters(), lr=0.001)\n",
        "\n",
        "    return enc, dec, enc_opt, dec_opt, criterion"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fk4Psg4uxMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_obj = get_model(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBjll8LHvM5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder1, decoder1, encoder_optimizer, decoder_optimizer, criterion = model_obj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mj7GR_vWU6y",
        "colab_type": "code",
        "outputId": "b3f2842f-742b-493d-97c9-cc0d30d2d2d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab.word2index['dog']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9oan39iyRoh",
        "colab_type": "code",
        "outputId": "a26f53f5-f829-4873-8056-a40d5befddbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loop(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 35s (- 46m 12s) (1000 1%) 1.9006\n",
            "1m 9s (- 44m 53s) (2000 2%) 1.7982\n",
            "1m 43s (- 44m 9s) (3000 3%) 1.7588\n",
            "2m 17s (- 43m 41s) (4000 5%) 1.6283\n",
            "2m 52s (- 43m 7s) (5000 6%) 1.5845\n",
            "3m 27s (- 42m 35s) (6000 7%) 1.5588\n",
            "4m 1s (- 42m 2s) (7000 8%) 1.4831\n",
            "4m 37s (- 41m 37s) (8000 10%) 1.4839\n",
            "5m 12s (- 41m 8s) (9000 11%) 1.4534\n",
            "5m 48s (- 40m 38s) (10000 12%) 1.4115\n",
            "6m 23s (- 40m 4s) (11000 13%) 1.3834\n",
            "6m 58s (- 39m 31s) (12000 15%) 1.3922\n",
            "7m 33s (- 38m 59s) (13000 16%) 1.3451\n",
            "8m 8s (- 38m 24s) (14000 17%) 1.3402\n",
            "8m 44s (- 37m 51s) (15000 18%) 1.2990\n",
            "9m 19s (- 37m 17s) (16000 20%) 1.3014\n",
            "9m 54s (- 36m 43s) (17000 21%) 1.2904\n",
            "10m 31s (- 36m 13s) (18000 22%) 1.2916\n",
            "11m 7s (- 35m 42s) (19000 23%) 1.3093\n",
            "11m 42s (- 35m 7s) (20000 25%) 1.2683\n",
            "12m 18s (- 34m 33s) (21000 26%) 1.2814\n",
            "12m 53s (- 33m 58s) (22000 27%) 1.3093\n",
            "13m 28s (- 33m 23s) (23000 28%) 1.2299\n",
            "14m 3s (- 32m 48s) (24000 30%) 1.2431\n",
            "14m 38s (- 32m 13s) (25000 31%) 1.2534\n",
            "15m 14s (- 31m 38s) (26000 32%) 1.2518\n",
            "15m 49s (- 31m 3s) (27000 33%) 1.2678\n",
            "16m 24s (- 30m 28s) (28000 35%) 1.2183\n",
            "16m 59s (- 29m 53s) (29000 36%) 1.2437\n",
            "17m 34s (- 29m 17s) (30000 37%) 1.2061\n",
            "18m 9s (- 28m 42s) (31000 38%) 1.2158\n",
            "18m 44s (- 28m 6s) (32000 40%) 1.2419\n",
            "19m 19s (- 27m 31s) (33000 41%) 1.2072\n",
            "19m 54s (- 26m 56s) (34000 42%) 1.2249\n",
            "20m 29s (- 26m 21s) (35000 43%) 1.2452\n",
            "21m 4s (- 25m 46s) (36000 45%) 1.2055\n",
            "21m 40s (- 25m 10s) (37000 46%) 1.2193\n",
            "22m 15s (- 24m 35s) (38000 47%) 1.1853\n",
            "22m 50s (- 24m 0s) (39000 48%) 1.1630\n",
            "23m 25s (- 23m 25s) (40000 50%) 1.2361\n",
            "24m 0s (- 22m 50s) (41000 51%) 1.2192\n",
            "24m 36s (- 22m 15s) (42000 52%) 1.2217\n",
            "25m 11s (- 21m 40s) (43000 53%) 1.1902\n",
            "25m 46s (- 21m 5s) (44000 55%) 1.1620\n",
            "26m 22s (- 20m 30s) (45000 56%) 1.1827\n",
            "26m 57s (- 19m 55s) (46000 57%) 1.2102\n",
            "27m 33s (- 19m 20s) (47000 58%) 1.1639\n",
            "28m 8s (- 18m 45s) (48000 60%) 1.1381\n",
            "28m 43s (- 18m 10s) (49000 61%) 1.1899\n",
            "29m 19s (- 17m 35s) (50000 62%) 1.1763\n",
            "29m 54s (- 17m 0s) (51000 63%) 1.1400\n",
            "30m 29s (- 16m 25s) (52000 65%) 1.1692\n",
            "31m 5s (- 15m 50s) (53000 66%) 1.1658\n",
            "31m 40s (- 15m 15s) (54000 67%) 1.1561\n",
            "32m 15s (- 14m 39s) (55000 68%) 1.2249\n",
            "32m 50s (- 14m 4s) (56000 70%) 1.1470\n",
            "33m 26s (- 13m 29s) (57000 71%) 1.2030\n",
            "34m 1s (- 12m 54s) (58000 72%) 1.1948\n",
            "34m 36s (- 12m 19s) (59000 73%) 1.1588\n",
            "35m 11s (- 11m 43s) (60000 75%) 1.1326\n",
            "35m 47s (- 11m 8s) (61000 76%) 1.1585\n",
            "36m 22s (- 10m 33s) (62000 77%) 1.1472\n",
            "36m 58s (- 9m 58s) (63000 78%) 1.1312\n",
            "37m 34s (- 9m 23s) (64000 80%) 1.1973\n",
            "38m 9s (- 8m 48s) (65000 81%) 1.1521\n",
            "38m 45s (- 8m 13s) (66000 82%) 1.2084\n",
            "39m 21s (- 7m 38s) (67000 83%) 1.1574\n",
            "39m 56s (- 7m 2s) (68000 85%) 1.1658\n",
            "40m 32s (- 6m 27s) (69000 86%) 1.1469\n",
            "41m 7s (- 5m 52s) (70000 87%) 1.1668\n",
            "41m 42s (- 5m 17s) (71000 88%) 1.1978\n",
            "42m 17s (- 4m 41s) (72000 90%) 1.1652\n",
            "42m 53s (- 4m 6s) (73000 91%) 1.1650\n",
            "43m 28s (- 3m 31s) (74000 92%) 1.1415\n",
            "44m 3s (- 2m 56s) (75000 93%) 1.1640\n",
            "44m 38s (- 2m 20s) (76000 95%) 1.1555\n",
            "45m 14s (- 1m 45s) (77000 96%) 1.1452\n",
            "45m 49s (- 1m 10s) (78000 97%) 1.1383\n",
            "46m 24s (- 0m 35s) (79000 98%) 1.1422\n",
            "47m 0s (- 0m 0s) (80000 100%) 1.1748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUmOXjlpQMSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model_params(encoder1, decoder1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1TOtN-Hhfp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}