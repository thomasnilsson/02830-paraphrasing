{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "paraphrasing-TRANSFORMER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ePg-a3ALIUf",
        "outputId": "c0baedd7-64bf-46f4-dd39-e125b9f2c012",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import spacy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "!pip3 install pytorch-nlp\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "\n",
        "google_path = '/content/gdrive/My Drive/colab/paraphrasing/'\n",
        "os.listdir(google_path)\n",
        "dataset_path = google_path + 'paraphrases.txt'\n",
        "picked_dataset_path = google_path + \"paraphrases_dataset.pickle\"\n",
        "encoder_path = google_path + 'encoder.model'\n",
        "decoder_path = google_path + 'decoder.model'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
            "\r\u001b[K     |███▋                            | 10kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 81kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.17.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0\n",
            "Device: cuda\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DtKV3xIrzFvk"
      },
      "source": [
        "# Data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MjvCiDaxyRn1",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalize(s):\n",
        "    s = re.sub(\"ß\", \"ss\", s)\n",
        "    s = re.sub(\"ä\", \"ae\", s)\n",
        "    s = re.sub(\"ö\", \"oe\", s)\n",
        "    s = re.sub(\"ü\", \"ue\", s)\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s.split(' ')\n",
        "\n",
        "def sub_unks(token_array, vocab):\n",
        "    words = set(vocab.index2word.values())\n",
        "    return [t if t in words else UNK for t in token_array ]\n",
        "\n",
        "def insert_tags(token_array):\n",
        "    return [SOS] + token_array + [EOS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gK4VONZtyRn9",
        "colab": {}
      },
      "source": [
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "UNK_token = 3\n",
        "\n",
        "PAD, SOS, EOS, UNK = \"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {}\n",
        "        self.n_words = 0\n",
        "        \n",
        "        self.add_word(PAD)\n",
        "        self.add_word(SOS)\n",
        "        self.add_word(EOS)\n",
        "        self.add_word(UNK)\n",
        "        \n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        for word in sentence:\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kIRqhrAro_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "def read_lines():\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(dataset_path).read().strip().split('\\n')\n",
        "    print('read %s lines' % len(lines))\n",
        "    return lines\n",
        "\n",
        "def prepare_data(limit=None):\n",
        "    lines = read_lines()\n",
        "    \n",
        "    # Limit dataset to a \"random\" subset\n",
        "    if limit:\n",
        "        idx = np.arange(len(lines))\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(idx)\n",
        "        idx = idx[:limit]\n",
        "        lines = [lines[i] for i in idx]\n",
        "        \n",
        "    print('Processing')\n",
        "    pairs = [l.split('|||') for l in lines]\n",
        "    pairs = [(x[1], x[2]) for x in pairs]\n",
        "    pairs = [(normalize(a), normalize(b)) for a,b in pairs]\n",
        "    pairs = [(insert_tags(a), insert_tags(b)) for a,b in pairs]\n",
        "\n",
        "    # Pick top N words\n",
        "    txt = [a+b for a,b in pairs]\n",
        "    txt_flat = flatten(txt)\n",
        "    freqs = nltk.FreqDist(txt_flat)\n",
        "    # find number of words occuring more than 2 times\n",
        "    N = np.sum(np.array(list(freqs.values())) > 2)\n",
        "    top = freqs.most_common(N)\n",
        "    top = [a for a,b in top]\n",
        "    \n",
        "    print('building vocab...')\n",
        "    vocab = Vocab()\n",
        "    \n",
        "    for w in top:\n",
        "        vocab.add_word(w)\n",
        "    \n",
        "    # Replace OOV words with UNK\n",
        "    pairs = [(sub_unks(a, vocab), sub_unks(b, vocab)) for a,b in pairs]\n",
        "    \n",
        "    #for s1, s2 in pairs:\n",
        "     #   vocab.add_sentence(s1)\n",
        "      #  vocab.add_sentence(s2)\n",
        "    \n",
        "    print(\"Counted words:\")\n",
        "    print(vocab.n_words)\n",
        "    print('Finished loading data')\n",
        "    return vocab, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5vzLF-uvzh-D"
      },
      "source": [
        "# Convert to Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IHTbaqf6yRoD",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def predict_single_sentence(src, tar, vocab):\n",
        "    # Insert into batch where all but the first entry just zeros\n",
        "    \n",
        "    src_batch = torch.zeros((src.size(0), BATCH_SIZE)).long().to(device)\n",
        "    tar_batch = torch.zeros((tar.size(0), BATCH_SIZE)).long().to(device)\n",
        "    src_batch[:, 0] = src[:, 0]\n",
        "    tar_batch[:, 0] = tar[:, 0]\n",
        "    \n",
        "    # Perform forward pass\n",
        "    output = model(src_batch, tar_batch, teacher_forcing_ratio=0) # no teacher forcing\n",
        "    \n",
        "    # Extract first predicted sentence\n",
        "    pred_token_ids = output[:, 0, :].argmax(dim=1)\n",
        "    \n",
        "    src_string = tensor_to_string(src.squeeze(), vocab)\n",
        "    tar_string = tensor_to_string(tar.squeeze(), vocab)\n",
        "    pred_string = tensor_to_string(pred_token_ids, vocab)\n",
        "    \n",
        "    print(\"SRC:\", src_string)\n",
        "    print(\"TAR:\", tar_string)\n",
        "    print(\"-\"*50)\n",
        "    print(\"PRED:\", pred_string)\n",
        "    return pred_token_ids\n",
        "\n",
        "\n",
        "def tensor_to_string(tensor, vocab, ignore_index=[PAD_token, UNK_token, SOS_token, EOS_token]):\n",
        "    tokens = [vocab.index2word[idx.item()] for idx in tensor if idx not in ignore_index]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def sentence_to_index(vocab, sentence):\n",
        "    return [vocab.word2index[word] for word in sentence]\n",
        "\n",
        "\n",
        "def sentence_to_tensor(vocab, sentence):\n",
        "    indexes = sentence_to_index(vocab, sentence)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def pair_to_tensor(pair):\n",
        "    src, tar = pair\n",
        "    input_tensor = sentence_to_tensor(vocab, src)\n",
        "    target_tensor = sentence_to_tensor(vocab, tar)\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def make_dataset(pairs, split_ratio=0.9):\n",
        "    # Shuffle dataset\n",
        "    n = len(pairs)\n",
        "    indices = np.arange(n)\n",
        "    np.random.seed(42)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    # Convert to tensors, use indices\n",
        "    tensor_pairs = [pair_to_tensor(pairs[i]) for i in indices]\n",
        "    \n",
        "    # Split dataset\n",
        "    split_idx = int(split_ratio * n)\n",
        "    train_data = tensor_pairs[:split_idx]\n",
        "    val_data = tensor_pairs[split_idx:]\n",
        "    return train_data, val_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3xcHl5gEyRoE",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "def load_dataset():\n",
        "    train_data, val_data, vocab = pickle.load(open(picked_dataset_path, \"rb\"))\n",
        "    return train_data, val_data, vocab\n",
        "\n",
        "def save_dataset(train_data, val_data, vocab):\n",
        "    dataset = (train_data, val_data, vocab)\n",
        "    pickle.dump(dataset, open(picked_dataset_path, \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwOHjgIqrpAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "'''Only pad when batch is loaded!'''\n",
        "def pad_collate(batch):\n",
        "    (xx, yy) = zip(*batch)\n",
        "    x_lens = [len(x) for x in xx]\n",
        "    y_lens = [len(y) for y in yy]\n",
        "\n",
        "    xx_pad = pad_sequence(xx, batch_first=False, padding_value=PAD_token)\n",
        "    yy_pad = pad_sequence(yy, batch_first=False, padding_value=PAD_token)\n",
        "\n",
        "    return xx_pad.squeeze(2), yy_pad.squeeze(2)\n",
        "\n",
        "'''Dataset class'''\n",
        "class ParaphraseDataset(object):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHcRWm6BrpAK",
        "colab_type": "code",
        "outputId": "48445f02-8d4f-4bf4-9b86-b6f809c21323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# define batch size\n",
        "BATCH_SIZE = 32\n",
        "print('Using batch size:', BATCH_SIZE)\n",
        "\n",
        "# use gpu if available, else use cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Loaded device:', device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using batch size: 32\n",
            "Loaded device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScxkJe0rpAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vocab, pairs = prepare_data(limit=200 * 1000)\n",
        "#train_data, val_data = make_dataset(pairs)\n",
        "#save_dataset(train_data, val_data, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BglytfGNDxR5",
        "colab_type": "text"
      },
      "source": [
        "# Load data from disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOWsAM5NDFvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, val_data, vocab = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EApMtTz_rpAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_iterator = DataLoader(ParaphraseDataset(train_data), batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
        "valid_iterator = DataLoader(ParaphraseDataset(val_data), batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8CP4LVKAMXJr"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZwpS9GTmMXsd",
        "colab": {}
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "    \n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "                \n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + Variable(self.pe[:,:seq_len], \\\n",
        "        requires_grad=False).cuda()\n",
        "        return x\n",
        "    \n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // heads\n",
        "        self.h = heads\n",
        "        \n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \n",
        "        bs = q.size(0)\n",
        "        \n",
        "        # perform linear operation and split into h heads\n",
        "        \n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        # transpose to get dimensions bs * h * sl * d_model\n",
        "       \n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "        # calculate attention using function we will define next\n",
        "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        # concatenate heads and put through final linear layer\n",
        "        concat = scores.transpose(1,2).contiguous()\\\n",
        "        .view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "    \n",
        "        return output\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "    \n",
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        self.eps = eps\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm\n",
        "    \n",
        "# build a decoder layer with two multi-head attention layers and\n",
        "# one feed-forward layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model).cuda()\n",
        "    \n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "            x2 = self.norm_1(x)\n",
        "            x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "            x2 = self.norm_2(x)\n",
        "            x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
        "            src_mask))\n",
        "            x2 = self.norm_3(x)\n",
        "            x = x + self.dropout_3(self.ff(x2))\n",
        "            return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model)\n",
        "        self.ff = FeedForward(d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x2 = self.norm_1(x)\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "    \n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "# we don't perform softmax on the output as this will be handled \n",
        "# automatically by our loss function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6G1TE-ZD5Oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import copy \n",
        "\n",
        "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "        \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output\n",
        "\n",
        "\n",
        "def create_masks(input_seq, target_seq):\n",
        "    input_pad = PAD_token\n",
        "    # creates mask with 0s wherever there is padding in the input\n",
        "    input_msk = (input_seq != input_pad).unsqueeze(1)\n",
        "    \n",
        "    # create mask as before\n",
        "    target_pad = PAD_token\n",
        "    target_msk = (target_seq != target_pad).byte().unsqueeze(1)\n",
        "    size = target_seq.size(1) # get seq_len for matrix\n",
        "    nopeak_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n",
        "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask) == 0).byte().to(device)\n",
        "    target_msk = target_msk & nopeak_mask\n",
        "    \n",
        "    return input_msk, target_msk\n",
        "\n",
        "# We can then build a convenient cloning function that can generate multiple layers:\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PG4rJToO3dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator):\n",
        "    ''' Evaluation loop for the model to evaluate.\n",
        "    Args:\n",
        "        model: A Seq2Seq model instance.\n",
        "        iterator: A DataIterator to read the data.\n",
        "        criterion: loss criterion.\n",
        "    Returns:\n",
        "        epoch_loss: Average loss of the epoch.\n",
        "    '''\n",
        "    #  some layers have different behavior during train/and evaluation (like BatchNorm, Dropout) so setting it matters.\n",
        "    model.eval()\n",
        "    # loss\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # no_grad() ensures parameters arent optimized (this would be cheating)\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, trg = batch\n",
        "            src = src.transpose(0,1)\n",
        "            trg = trg.transpose(0,1)\n",
        "            trg_input = trg[:, :-1]\n",
        "            src_mask, trg_mask = create_masks(src, trg_input)\n",
        "            \n",
        "            # Forward pass\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            # Calculate loss\n",
        "            preds_ = preds.view(-1, preds.size(-1))\n",
        "            targets = trg[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(preds_, targets)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpLEmGaePMfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_one_epoch(print_every=100):\n",
        "    # Toggle train mode to adjust params\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(train_iterator):\n",
        "        # Prepare batch\n",
        "        src, trg = batch\n",
        "        src = src.transpose(0,1)\n",
        "        trg = trg.transpose(0,1)\n",
        "        trg_input = trg[:, :-1]\n",
        "        src_mask, trg_mask = create_masks(src, trg_input)\n",
        "\n",
        "        # Forward pass\n",
        "        preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "        # Perform backprop\n",
        "        optim.zero_grad()\n",
        "        preds_ = preds.view(-1, preds.size(-1))\n",
        "        targets = trg[:, 1:].contiguous().view(-1)\n",
        "        loss = criterion(preds_, targets)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "        # Save loss for epoch\n",
        "        epoch_loss += loss.item()\n",
        "            \n",
        "    return epoch_loss / len(train_iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en5E7N_8Paoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_model = 512\n",
        "heads = 8\n",
        "N = 6\n",
        "src_vocab = vocab.n_words\n",
        "trg_vocab = vocab.n_words\n",
        "model = Transformer(src_vocab, trg_vocab, d_model, N, heads).to(device)\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "# this code is very important! It initialises the parameters with a\n",
        "# range of values that stops the signal fading or getting too big.\n",
        "# See this blog for a mathematical explanation.\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGpWIc74rpA_",
        "colab_type": "code",
        "outputId": "662c199b-6cbd-4006-bf00-4e4c3281b29e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import time\n",
        "import pickle\n",
        "MODEL_SAVE_PATH = google_path + 'transformer-model-paraphrase.pt'\n",
        "LOSSES_SAVE_PATH = google_path + 'paraphrase-losses.pickle'\n",
        "print('saving to:', MODEL_SAVE_PATH, LOSSES_SAVE_PATH)\n",
        "val_losses = []\n",
        "train_losses = []\n",
        "best_val_loss = np.infty"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving to: /content/gdrive/My Drive/colab/paraphrasing/transformer-model-paraphrase.pt /content/gdrive/My Drive/colab/paraphrasing/paraphrase-losses.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5fkdImhBM3Np",
        "outputId": "045f12bf-e9f3-40e2-85a1-05a90a4f8c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "for epoch in range(50):\n",
        "    start_time = time.time()\n",
        "    # Run one epoch\n",
        "    train_loss = train_one_epoch()\n",
        "    \n",
        "    # Evaluate \n",
        "    val_loss = evaluate(model, valid_iterator)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('Epoch [%i] finished after [%i] seconds, val loss: [%.3f]' % \n",
        "          (epoch+1, elapsed_time, val_loss))\n",
        "    \n",
        "    # Save losses\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    pickle.dump((train_losses, val_losses), open(LOSSES_SAVE_PATH, 'wb'))\n",
        "    \n",
        "    # If generalization error improved then save the model to disk\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1] finished after [487] seconds, val loss: [2.507]\n",
            "Epoch [2] finished after [482] seconds, val loss: [2.182]\n",
            "Epoch [3] finished after [481] seconds, val loss: [2.026]\n",
            "Epoch [4] finished after [480] seconds, val loss: [1.937]\n",
            "Epoch [5] finished after [481] seconds, val loss: [1.888]\n",
            "Epoch [6] finished after [482] seconds, val loss: [1.855]\n",
            "Epoch [7] finished after [488] seconds, val loss: [1.841]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgTw7W4pPfAF",
        "colab_type": "text"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bev75CsOKA1",
        "colab_type": "code",
        "outputId": "8a315eee-6c03-4e9b-cf42-e9df98081c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRPVzOZ4PgLF",
        "colab_type": "text"
      },
      "source": [
        "# BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6iU7aZgPi7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#n_valid = len(valid_data.examples)\n",
        "#valid_src_txt = [valid_data.examples[i].src for i in range(n_valid)]\n",
        "#valid_trg_txt = [valid_data.examples[i].trg for i in range(n_valid)]\n",
        "#valid_src_idx = [tokens_to_tensor(SRC, valid_src_txt[i]) for i in range(n_valid)]\n",
        "#valid_trg_idx = [tokens_to_tensor(TRG, valid_trg_txt[i]) for i in range(n_valid)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecj3wwHJTSIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKV-AowqhH2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_tensor(LANG, tokens, reverse=False):\n",
        "    idx = [LANG.word2index[t] for t in tokens]\n",
        "    if reverse: idx = list(reversed(idx))\n",
        "    return idx\n",
        "\n",
        "def remove_tags(LANG, idx):\n",
        "    ignore = [LANG.word2index[PAD], LANG.word2index[SOS], LANG.word2index[EOS]]\n",
        "    return [i for i in idx if i not in ignore]\n",
        "\n",
        "\n",
        "def remove_duplicates(vocab, id_array):\n",
        "    # remove tokens which are repeated in succession\n",
        "    id_array = [id_array[i] for i in range(len(id_array)) if i == 0 or id_array[i-1] != id_array[i]]\n",
        "    return id_array\n",
        "\n",
        "def tensor_to_tokens(LANG, tensor, reverse=False):\n",
        "    tensor = remove_tags(LANG, tensor)\n",
        "    tensor = remove_duplicates(LANG, tensor)\n",
        "    tokens = [LANG.index2word[x] for x in tensor]\n",
        "    tokens = list(reversed(tokens)) if reverse else tokens\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def beautify_token_array_to_str(token_array):\n",
        "    import re\n",
        "    # Convert to string, strip training spaces\n",
        "    string = ' '.join(token_array).strip()\n",
        "    '''Replaces common contractions such as don't, ain't, isn't etc.'''\n",
        "    string = re.sub('ai n t', \"ain't\", string)\n",
        "    string = re.sub('do n t', \"don't\", string)\n",
        "    string = re.sub('is n t', \"isn't\", string)\n",
        "    string = re.sub('i m', \"i'm\", string)\n",
        "    string = re.sub('was n t', \"wasn't\", string)\n",
        "    string = re.sub('wo n t', \"won't\", string)\n",
        "    string = re.sub('should n t', \"shouldn't\", string)\n",
        "    return string\n",
        "\n",
        "                         \n",
        "def get_pred_tar_pairs(batch_iter):\n",
        "    'predict on batches and finds the untouched txt sentence which matches the target.'\n",
        "    TARS = []\n",
        "    PREDS = []\n",
        "    SRCS = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_iterator:\n",
        "            # Prepare batch\n",
        "            src, trg = batch\n",
        "            src = src.transpose(0,1)\n",
        "            trg = trg.transpose(0,1)\n",
        "            trg_input = trg[:, :-1]\n",
        "            src_mask, trg_mask = create_masks(src, trg_input)\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            # Argmax over vocab and get the predicted tokens\n",
        "            pred = output.argmax(dim=2)\n",
        "\n",
        "            SRCS += [tensor_to_tokens(vocab, x.tolist()) for x in src]\n",
        "            TARS += [tensor_to_tokens(vocab, x.tolist()) for x in trg]\n",
        "            PREDS += [tensor_to_tokens(vocab, x.tolist()) for x in pred]\n",
        "    \n",
        "    return SRCS, TARS, PREDS\n",
        "\n",
        "from torchnlp.metrics import get_moses_multi_bleu\n",
        "def bleu(tar, pred): \n",
        "    'Calculates moses bleu given two arrays of str tokens'\n",
        "    tar, pred = ' '.join(tar), ' '.join(pred)\n",
        "    return get_moses_multi_bleu([tar], [pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6oBzkniP6oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRCS, TARS, PREDS = get_pred_tar_pairs(valid_iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsAO7ZwPK0PP",
        "colab_type": "code",
        "outputId": "2b4c5ee2-4eb6-4b65-8878-133c4fd0a5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "indices = np.arange(len(SRCS))\n",
        "choices = [np.random.choice(indices) for i in range(10)]\n",
        "choices"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6414, 2134, 8739, 9936, 16775, 10006, 11526, 9350, 431, 12816]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ZSdw5DK8U0",
        "colab_type": "code",
        "outputId": "c7137ab8-7036-4b61-8869-ca10e0d28c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "for i in choices:\n",
        "    x,y,z = SRCS[i], TARS[i], PREDS[i]\n",
        "    print('[src]', beautify_token_array_to_str(x))\n",
        "    print('[tar]', beautify_token_array_to_str(y))\n",
        "    print('[out]', beautify_token_array_to_str(z))\n",
        "    print()\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[src] they apply\n",
            "[tar] they are implemented\n",
            "[out] they are applicable\n",
            "\n",
            "[src] nations regional commissions\n",
            "[tar] regional commission\n",
            "[out] levels\n",
            "\n",
            "[src] yes i am\n",
            "[tar] i ve got it\n",
            "[out] yeah m got it\n",
            "\n",
            "[src] are from\n",
            "[tar] emerging from\n",
            "[out] are from\n",
            "\n",
            "[src] last fiscal year\n",
            "[tar] most recently completed fiscal year\n",
            "[out] last recent year\n",
            "\n",
            "[src] now wait\n",
            "[tar] are waiting\n",
            "[out] waiting for\n",
            "\n",
            "[src] please clarify whether\n",
            "[tar] clarify whether\n",
            "[out] be whether\n",
            "\n",
            "[src] that issue\n",
            "[tar] the above issues\n",
            "[out] that question\n",
            "\n",
            "[src] possibility of establishing a\n",
            "[tar] possibility of creating an\n",
            "[out] possibility of developing a\n",
            "\n",
            "[src] designed to improve\n",
            "[tar] for improving the\n",
            "[out] to improving\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ_SJX9NVDMH",
        "colab_type": "code",
        "outputId": "002d1f95-167b-4163-f029-742b0faba810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install easy-rouge"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting easy-rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/0a/b7ebb887dac3ece27fffc65bbc7dc0abcf991f2ccce8073126329ce4be8f/easy_rouge-0.2.2-py3-none-any.whl\n",
            "Installing collected packages: easy-rouge\n",
            "Successfully installed easy-rouge-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W4XJMj9VfPm",
        "colab_type": "text"
      },
      "source": [
        "# ROUGE scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArYTyggLUrcc",
        "colab_type": "code",
        "outputId": "3a2217a7-77b9-42ca-f827-7bda13b92635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from rouge.rouge import rouge_n_sentence_level\n",
        "\n",
        "def calc_rouge(pred, tar):\n",
        "    _, _, rouge1 = rouge_n_sentence_level(pred, tar, 1)\n",
        "    _, _, rouge2 = rouge_n_sentence_level(pred, tar, 2)\n",
        "    _, _, rouge3 = rouge_n_sentence_level(pred, tar, 3)\n",
        "    return rouge1*100, rouge2*100, rouge3*100\n",
        "\n",
        "def plot_rouge(scores, title):\n",
        "    scores = np.array(scores)\n",
        "    plt.hist(scores, bins=100, edgecolor='black')\n",
        "    plt.title(title)\n",
        "    plt.savefig(google_path + title + '.png')\n",
        "    plt.show()\n",
        "    print('AVG:', np.mean(scores))\n",
        "\n",
        "# Calculate ROUGE-N for an example.\n",
        "recall, precision, rouge = rouge_n_sentence_level(TARS[0], PREDS[0], 1)\n",
        "print('ROUGE-2-R', recall)\n",
        "print('ROUGE-2-P', precision)\n",
        "print('ROUGE-2-F', rouge)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROUGE-2-R 0.8333333333333334\n",
            "ROUGE-2-P 0.8333333333333334\n",
            "ROUGE-2-F 0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoofsNxMWhHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate rouge-N for all validation examples\n",
        "rouge_scores = [calc_rouge(TARS[i], PREDS[i]) for i in range(len(TARS))]\n",
        "pickle.dump(rouge_scores, open(google_path + 'rouge_scores.pickle', 'wb'))\n",
        "r1, r2, r3 = zip(*rouge_scores)\n",
        "plot_rouge(r1, 'ROUGE-1'), plot_rouge(r2, 'ROUGE-2'), plot_rouge(r3, 'ROUGE-3')\n",
        "np.mean(r1), np.mean(r2), np.mean(r3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epZsPQLD6Siw",
        "colab_type": "text"
      },
      "source": [
        "# Predict single sentence (used for web app)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MramdBMHdTc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_string(s):\n",
        "    # ex s = 'and the evaluation'\n",
        "    tokens = normalize(s)\n",
        "    tokens = insert_tags(tokens)\n",
        "    return tokens\n",
        "\n",
        "def paraphrase(src, max_len=80):\n",
        "    model.eval()\n",
        "    src = tokenize_string(src)\n",
        "    src = Variable(torch.LongTensor([[vocab.word2index[tok] for tok in src]])).to(device)\n",
        "    \n",
        "    # Create mask and encode src\n",
        "    src_mask = (src != PAD_token).unsqueeze(-2)\n",
        "    encoder_outputs = model.encoder(src, src_mask)\n",
        "    \n",
        "    # Create decoder output as SOS tokens\n",
        "    outputs = torch.zeros(max_len).type_as(src.data)\n",
        "    outputs[0] = torch.LongTensor([vocab.word2index[SOS]])\n",
        "        \n",
        "    # Predict each token untill EOS\n",
        "    for i in range(1, max_len):    \n",
        "        trg_mask = np.triu(np.ones((1, i, i)), k=1).astype('uint8')\n",
        "        trg_mask = Variable(torch.from_numpy(trg_mask) == 0).to(device)\n",
        "        \n",
        "        # Predict \n",
        "        out = model.out(model.decoder(outputs[:i].unsqueeze(0), encoder_outputs, src_mask, trg_mask))\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        # Argmax over outputs\n",
        "        outputs[i] = out[:, -1].argmax().item()\n",
        "        \n",
        "        # Check if EOS\n",
        "        if outputs[i] == vocab.word2index[EOS]: \n",
        "            break\n",
        "\n",
        "    # Convert to tokens, and lastly to beautiful string\n",
        "    sentence = tensor_to_tokens(vocab, outputs[:i].tolist())\n",
        "    return beautify_token_array_to_str(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tvUQkjwfDgv",
        "colab_type": "code",
        "outputId": "775b1006-4da0-40bf-9c61-884e4545e7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "paraphrase('this is essential')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it is crucial'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-jMs3VQtgyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(vocab, open(google_path + 'vocab-200k.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJfhda67001T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}